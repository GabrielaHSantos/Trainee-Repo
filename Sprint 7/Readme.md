# Resumo

### Spark com PySpark

Primeiro, foi passado o ambiente necessário pra rodar o Spark e como instalar tudo certo. A gente configurou o Ubuntu numa VM, baixou e instalou o Spark, adicionou algumas bibliotecas e rodou uns exemplos. Também baixamos os dados de exemplo pra usar depois.
Depois disso, vimos sobre RDD, Dataset e DataFrame. Aprendemos a diferença entre eles, como usar cada um e como transformar um no outro. Também foi passado as principais ações e transformações que dá pra fazer nos dados, como importar e exportar arquivos e manipular as informações.

A parte de Spark SQL foi sobre como criar bancos de dados e tabelas dentro do Spark. Vimos a diferença entre tabelas gerenciadas e externas, aprendemos a criar views, fizemos comparações entre DataFrames e SQL e trabalhamos com joins de diferentes tipos, tanto no SQL quanto usando DataFrames. Depois, entramos na parte de criação de aplicações. Fizemos aplicações que escrevem no console, usamos parâmetros na linha de comando e desenvolvemos um conversor de formatos de arquivos. Também teve umas atividades pra testar o que foi aprendido.Por último, vimos a parte de otimização. Foi passado sobre particionamento e bucketing, cache e persistência, explicando como essas coisas ajudam a melhorar o desempenho e evitar processamentos desnecessários.

# Exercícios

Exercício tmdb clique [aqui](../Sprint%207/Exercicios/exerciciotmdb/)

Exercício Spark (contador) clique [aqui](../Sprint%207/Exercicios/exerciciosparkcontador.md)

Exercício Lab Glue clique [aqui](../Sprint%207/Exercicios/exerciciolabglue/)

# Evidências

Acesse as *`evidencias`* dos exercicios clicando [aqui]()

Acesse as *`evidencias`* do desafio clicando [aqui]()

# Certificados

Nesta Sprint não houveram *certificados* por fora da Udemy.